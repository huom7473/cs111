NAME: Michael Huo
EMAIL: EMAIL
ID: UID

>>>>INCLUDED FILES<<<<

The following files have been included as per the spec's requirements:

SortedList.h: 
supplied header file, left unmodified for use with list part of lab

SortedList.c: 
C source module that contains implementation of interface specified in SortedList.h

lab2_list.c: 
C source module that uses the doubly linked list implementation/interface, used to generate
lab2_list executable for use in rest of project

Makefile:
Makefile that includes necessary targets (as well as some other stuff needed to make the targets work as intended/my own use)
as specified.
Note: make dist will re-run tests, generate graphs, etc. every time it's called, even if the source modules have not changed.
Without doing this, the sanity check fails (at least for project 2A, so presumably for this one as well)
This was discussed (somewhat) on Piazza @372, and @378.

lab2b_list.csv:
test results (obtained from make tests) in CSV format, generated from lab2_list executable

profile.out:
profiling report with data as mandated in the spec. Generated by make profile (using gperftools), which largely
copies the format of commands shown in discussion section tutorials. Outlines where CPU time was spent in an
unpartitioned spin lock run.

Images in the form lab2b_#.png:
Graphs generated using gnuplot script lab2b.gp. More on script below.
I have no better explanation for them other than what is detailed in the spec:

    lab2b_1.png ... throughput vs. number of threads for mutex and spin-lock synchronized list operations.
    lab2b_2.png ... mean time per mutex wait and mean time per operation for mutex-synchronized list operations.
    lab2b_3.png ... successful iterations vs. threads for each synchronization method.
    lab2b_4.png ... throughput vs. number of threads for mutex synchronized partitioned lists.
    lab2b_5.png ... throughput vs. number of threads for spin-lock-synchronized partitioned lists.

README:
this README file.

The following files were not explicitly mandated by the spec:

lab2b.gp:
gnuplot script I made (using the provided project 2a .gp scripts as a base and mainly just copying the format)
to use the data in lab2b_list.csv to produce all five required graphs mentioned above.
The script is called by make graphs.

test.sh:
A shell script I use to run appropriate tests for use in the graphs. More info on the tests below.
The script will discard stderr output for all tests, to avoid clogging the terminal with error messages from
the lab2_list program (which print due to list corruption).
test.sh is called by make tests.


>>>>END OF INCLUDED FILES<<<<

>>>>NOTES<<<<

Note on tests performed/data collected:
Although the spec enumerated all of the tests to be ran for graphing, I found that sometimes it was either
slightly vague, or the data seemed a little bit incomplete when fed through the test scripts. One notable example
is the fact that failures weren't very consistent when running only 16 threads/iterations in the 4 list unprotected
tests, even though the spec doesn't say to go above that. I added 32 and 64 iteration runs to see more consistent
fails at certain levels.

So, in some cases, the test.sh script performs tests beyond what is explicitly stated, and thus the graphs may have
data that wouldn't otherwise be generated by only running the tests in the spec. Most of the time, it was just so that
the graphs look a little better.

I did not, however, try to fix every instance of a possibly misleading graph.
For example, in lab2b_3.png, it may seem like the synchronization options resulted in failed runs below 10 iterations, even
though it is only a reuslt of no tests being run for them. It also looks like there were no successful mutex runs. This is beacuse
the spin points completely overlap the mutex points. I tried messing around with different colors, to no avail. The correct
data is still in the csv, it's just not visible in the plots.

Note on locking options:
The spec mentions "number of lock operations" to divide by when computing an average wait-for-lock time. I assumed that this is
actual lock acquisitions, not locks + unlocks. So, I used the number (threads * iterations * 2) + (threads * lists), since
each thread locked once for each insertion/lookup-deletion, and once for each sublist to determine the length. It passed the
sanity check, so I assume it's acceptable.
Additionally, the spec is quite vague on what to do when checking the lengths of all the sublists when you have multiple. It says
to figure out how to do it "safely and correctly." Correctness here could be interpreted in multiple ways, so I chose to make sure
only that no other threads would interfere with a sublist that was being examined by another, but other threads would be able to
still modify other sublists while one sublist was still being examined. This makes each sublist length check an atomic operation. 
The alternative to this would be to acquire all the locks for all the sublists and make sure the entire huge list was locked down
before any length checking started, but I figured this wasn't necessarily more "correct," and it would result in a much larger
performance loss, since all the other threads would have to sit idle while one thread checks all the sublists one by one. This 
isn't worth it to me, especially since the main goal of length checking was to check for corruption, anyway - we aren't as worried
about the exact length at every point.

Note on hashing:
The spec only says to use a modulo hash on the key, so I just used the hash of the first character (byte) in the key, since that's the only
one that's guaranteed to exist, and it's a quick/simple operation.
This limits the max number of lists to 255, essentially, but I figured that wouldn't be a problem.

>>>>END OF NOTES<<<<

>>>>QUESTIONS<<<<<

2.3.1 - CPU time in the basic list implementation
Most of the CPU time in the 1/2 thread list tests are probably spent in the sorted list lookup functions. This is because since there is
very little contention (no contention, in the case of one thread) for the locks, we probably spend more time doing actual work
than waiting/getting blocked while a lock is held by another thread. However, since there is still only one list, we might see something
like a 50/50 split in the 2 thread scenario, where one thread is always waiting for the other. This will still be a lot higher than
what I expect from higher thread counts.
As the thread count grows higher and contention increases drastically as a result, we will spend a lot more time in the spin loop
for the spin lock tests, effectively burning CPU cycles while waiting for the lock to be released. I expect to see the vast majority
of thread time being spent spinning in the while loop with high thread count.
However, since in the case of the mutex lock, the thread blocks while waiting for the lock, a lot less time will be spent in the
actual locking functions, and most of the time will be spent doing the insertions/lookups that require list traversal, as that
takes the most amount of time in the thread code.

2.3.2 - Execution Profiling
The spin lock program, as expected, spends most of its time in the spinning while loop section of code when the threadcount
is large. This is the case, and we see a very expensive operation because higher contention between threads over one bottlenecked
resource (just one list to perform actions on) leads to a higher probability that the lock is acquired by another thread
at any given time (since there are more threads in total), and more CPU cycles will be burnt while waiting for other threads
to release the lock.

2.3.3 - Mutex Wait Time
The average lock-wait time increases dramatically because more contending threads means a higher chance that the lock is already held
when a thread tries to acquire it, and that means it has to pay the overhead of blocking/waiting in a possibly very long queue
(the size of which increases with thread count) a lot more often. The effects of blocking and waiting longer each time the thread blocks
makes the average wait time rise very dramatically.
The time per operation rises less dramatically because the bottleneck (list lock) doesn't narrow with more threads - you are simply just wasting more
time switching between threads. The same amount of throughput can THEORETICALLY be achieved with increased number of threads, since
at least one thread can still be doing work on the list at a time, which is the same situation as a low thread run.
The wait time can go higher than the completion time per operation because it is the average wait time of multiple threads. If you have one thread
doing work at a time, and 10 threads waiting, you can get the same work done as if only one thread was doing all of the work (with 0 wait time, since
there's no contention), but the wait time inflates rapidly because in this scenario, wait time is increasing by 10 ns for every 1 ns of real time
that passes (since there's 10 threads waiting), and each thread individually has to wait longer to do its work as well - the last thread in the wait queue
will have to wait a very long time for every other thread to finish waiting and doing their work.

2.3.4 - Performance of Partitioned Lists
The throughput increases with the amount of lists becuase the bottleneck widens. Since there's more lists to put things in, there's a smaller chance
that the lock on a list is already held by another thread. Additionally, multiple threads can edit the large list at a time, since there's more lists.
The throughput will continue increasing as the number of lists is further increased up to a point for the same reason, 
but eventually it will cap out as the number of lists grows too large to be taken advantage of (when list count >> thread count), and the primary
bottleneck is the speed of actual execution of instructions rather than waiting time on locks to update a list.
I'm pretty confused as to what the last part of the question is asking. After thinking about it for some time, I think the most reasonable
interpretation would be "The throughput of an N-way partitioned list with x amount of threads is equivalent to the throughput
of 1 list with (1/n) * x threads." i.e. the number of threads per list stays the same.
So, I will answer this part of the question with the above interpretation.
This suggestion does not hold in the graphs, except for maybe in the case of 1 thread/1 list vs. 4 threads/4 lists. We see that if we increase
the number of lists and threads together, there is an increase in throughput (which will be bounded, as I explained above). For example,
the 16 threads/16 lists has at least 3-4x the throughput of the 1 thread/1 list (a little bit hard to tell due to the log-scaled y axis). This
is because multiple threads can be scheduled at the same time, since the systems we run these tests on are multi-core. That means multiple lists
will be able to be updated at a time, and with larger thread/list counts, the overhead increase for adding, managing, and scheduling threads,
and suffering through higher contention is outweighed by the increase in total throughput offered by multi-core parallelism.

