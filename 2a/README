NAME: Michael Huo
EMAIL: EMAIL
ID: UID

>>>>INCLUDED FILES<<<<

The following files have been included as per the spec's requirements:

lab2_add.c: 
C source module that compiles into lab2_add for use in rest of project

SortedList.h: 
supplied header file, left unmodified for use with list part of lab

SortedList.c: 
C source module that contains implementation of interface specified in SortedList.h

lab2_list.c: 
C source module that uses the doubly linked list implementation/interface, used to generate
lab2_list executable for use in rest of project

Makefile:
Makefile that includes necessary targets (as well as some other stuff needed to make the targets work as intended/my own use)
as specified.
Note: make dist will re-run tests, generate graphs, etc. every time it's called, even if the source modules have not changed.
This was discussed (somewhat) on Piazza @372, and @378.

lab2_add.csv:
test results (obtained from make tests) in CSV format, generated from lab2_add executable

lab2_list.csv:
test results (obtained from make tests) in CSV format, generated from lab2_list executable

Images in the form lab2_[add/list]-#.png:
Graphs generated using gnuplot scripts provided. More on scripts below.
I have no better explanation for them other than what is detailed in the spec:

    For part 1 (lab2_add):
        lab2_add-1.png ... threads and iterations required to generate a failure (with and without yields)
        lab2_add-2.png ... average time per operation with and without yields.
        lab2_add-3.png ... average time per (single threaded) operation vs. the number of iterations.
        lab2_add-4.png ... threads and iterations that can run successfully with yields under each of the synchronization options.
        lab2_add-5.png ... average time per (protected) operation vs. the number of threads.
    For part 2 (lab2_list):
        lab2_list-1.png ... average time per (single threaded) unprotected operation vs. number of iterations (illustrating the correction of the per-operation cost for the list length).
        lab2_list-2.png ... threads and iterations required to generate a failure (with and without yields).
        lab2_list-3.png ... iterations that can run (protected) without failure.
        lab2_list-4.png ... (length-adjusted) cost per operation vs the number of threads for the various synchronization options.

The following files were not explicitly mandated by the spec:

lab2_add.gp:
Provided gnuplot script with minor modifications (to some set xrange commands) to make graphs look a little nicer
when ran on my system. For some reason, even though I have the same version of gnuplot on my system as the one
on lnxsrv09, the graphs generated have slightly different x ranges in some cases. Sometimes, on my system that meant
there were points on the very right edge of the graph, where they were harder to see. The modifications
address that.
lab2_add.gp plots the graphs required in the lab2_add section of the project, and is called by make graphs.

lab2_list.gp:
Provided gnuplot with similar changes as described above.
lab2_list.gp plots the graphs required in the lab2_list section of the project, and is called by make graphs.

test.sh:
A shell script I use to run appropriate tests for use in the graphs. More info on the tests below.
The script will discard stderr output for all tests, to avoid clogging the terminal with error messages from
the lab2_list section.
test.sh is called by make tests.

README:
this README file.

>>>>END OF INCLUDED FILES<<<<

>>>>NOTES<<<<

Note on tests performed/data collected:
Although the spec enumerated all of the tests to be ran for graphing, I found that sometimes it was either
slightly vague, or the data seemed a little bit incomplete when fed through the test scripts (for example, 
only running lab2_add with 10000 iterations when testing yield/synchronization options made it look on the graph
like all the other tests failed).

So, in some cases, the test.sh script performs tests beyond what is explicitly stated, and thus the graphs may have
data that wouldn't otherwise be generated by only running the tests in the spec. However, I made sure that each time
I did tests beyond the spec, it either made the graphs look more complete or easier to see patterns in. In other words, 
I believe the graphs generated are improved as a result of the extra tests.

I did not, however, try to fix every instance of a possibly misleading graph as a result of holes in the tests specified.
For example, in add-4, it may seem like the synchronization options resulted in failed runs below 100 iterations, even
though it is only a reuslt of no tests being run for them.

Note on sync option in lab2_list:
The spec does not comment (unless I'm missing something) on how long each thread should hold a lock for regaridng the locking mechanism to 
be implemented in lab2_list. I could've made the lock be applied and released once for all the operations, once per lookup loop/delete loop, 
once per each lookup/delete, etc. 
I decided that since the lock would have to be on the whole list anyway, and the spec
mentions, somewhat ambiguously, "one set of operations" to be protected, to have a thread hold the lock on the list
for all of its insertions and deletions, as any smaller time would probably only result in more overhead from having to lock/unlock so many times
with no real gain. Additionally, the part in the spec mentioning division by iterations/4 also makes it sound like it is correct to do so,
as the list could grow much longer with a shorter locking period (e.g. all threads do their insertions before any start their deletions)

In case it is worth mentioning, much of the thread-related code was based on code either in the textbook readings or in the tutorial
linked in the spec. I tried to comment as much of my code as possible to improve readability.

>>>>END OF NOTES<<<<

>>>>QUESTIONS<<<<<

2.1.1 - causing conflicts
The behavior of this threaded program is nondeterministic, and errors are seen as a result of improper handling of the race condition/
critical section. In order for the sum to be wrong, a thread needs to be interrupted (preempted) at some intermediate state, like after
having calculated the sum but before loading the counter with the new sum. If there are less iterations to be done, there is a
smaller chance that the thread is preempted in the middle of one - it could even finish all of its iterations in one time slice,
making it impossible to be interrupted in a critical section.

2.1.2 - cost of yielding
The yield runs are slower because of the costs of context switching. Even though context switching between threads may take less time
than context switching a process, there is still a lot of overhead for each switch. So, if we cause a context switch by yielding
every iteration of add, there will be a lot more time wasted. It is not possible to get valid per-operation timings if we're interested
in realistic timings, because the costs of context switching depend on numerous factors out of our control and which are also
extremely difficult to quantify/measure properly, such as what the previous process/thread was on a core before dispatching another one.

2.1.3 - measurement errors
The average cost per operation drops because creating/maintaining each thread has a sizable amount of overhead, like having to set
up a separate stack/program counter, scheduling, etc. This overhead is more or less constant, so with a small amount of iterations,
it will account a large amount of the total time taken. The cost per iteration drops in somewhat of an exponential decay - it will 
never go below the time it takes to execute the machine intructions in the compiled code (barring multi-core parallelism, but the number
of cores is also finite). So, we can pick a point at the "elbow" of the exponential curve, where the slope begins to approach 0,
to see a near-minimum cost per operation while keeping iteration count reasonable.

2.1.4 - costs of serialization
For low numbers of threads, the limitations of the locking mechanisms won't show as much. For example, if only two threads are
competing for a resource, they probably won't spend that much time waiting for a lock and trying to lock it, since it's going to get the lock
when the other thread releases it. However, with large amounts of threads, the time spent executing intructions related to the
locking mechanism increases - with 10 threads and only one resource, the threads have a much lower chance of getting the lock,
and spend more time waiting. If the lock mechanism is inefficient, more time will be wasted trying to manage locks.

2.2.1 - scalability of Mutex
In the add graph, we see the cost increasing with threads, then remaining roughly constant/maybe slightly decreasing as thread count
grows further (could be within margin of error). This is because more threads are competing for the same resource, and thus spending more
time managing the lock rather than doing "useful" computations. In other words, we pay extra overhead for not too much gain.
In the list graph, the cost per operation is almost exactly constant. This might be because the thread holds each lock for almost all of
its operations, so it doesn't spend as much time trying to lock/unlock in order to update the list and we pay less overhead switching
around to update one resource.

2.2.2 - scalability of spin locks
In both list and add, we see the spin lock overhead increasing fairly rapidly with thread count, faster than the increase in overhead
of the mutex locks. This is because spin locking is a more inefficient way of securing a lock.
Spin locks waste a lot more cycles waiting for locks, and each thread will often use up its time slice
executing instructions that don't do anything (spinning) other than waste CPU cycles. This is especially apparent in the lab2_list
tests, since the threads hold their locks for longer, so other threads are likely to spend more time spinning before they're able
to get the lock.









